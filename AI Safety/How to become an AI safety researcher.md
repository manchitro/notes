[source: original article](https://www.lesswrong.com/posts/uKPtCoDesfawNfyJg/how-to-become-an-ai-safety-researcher) 
# Paths into AI safety
## What degrees did people get?
- Mostly Mathematics and Computer Science.
- Research as undergrads in related fields
- [[Effective Altruism]] and [[Rationality as a discipline]]
- Additional Reading:
	- [[superintelligence-paths-dangers-strategies-by-nick-bostrom.pdf|Superintelligence]]
	- [PhD or programming? ](https://80000hours.org/podcast/episodes/olsson-and-ziegler-ml-engineering-and-safety/) 
	- [[human_compatible.pdf|Human Compatible]]
	- [[Christian-Alignment-Problem-Intro-and-Ch1.pdf|The Alignment Problem]]
## Skills
### Technical skills
- programming
- mathematics (for theoretical work)
	- linear algebra
	- probability
	- calculus
- train/debug models and run experiments (for practical work)
- Reinforcement Learning
- Generally pytorch, numpy, pandas and matplotlib
- For NLP, [**Huggingface transformers** library](https://huggingface.co/docs/transformers/en/index)
- NLP safety work involves collecting data from humans
	- which means that frontend software development skills (**typescript, react**) are useful for making data collection websites
- For theoretical safety work:
	- coding and math is useful regardless
	- thorough knowledge over a reasonably broad range
		- ```"There is a mathematical skill of “given that I have this conjecture, how do I state a proof?”. Having knowledge of a lot of math can help with this because seemingly random math facts may be useful for constructing a proof."```
		- Economics
		- Certain parts of psychology
		- Areas of philosophy, specifically in philosophy of science, meaning and reference, knowledge (although this might be a minority view)
		- Dynamical systems and equilibria (possibly related to biology)
		- Cybernetics and information theory
	- A definition of 'good' on a societal level
	- Also read Andrew Critch’s [post on power dynamics](https://www.lesswrong.com/posts/WjsyEBHgSstgfXTvm/power-dynamics-as-a-blind-spot-or-blurry-spot-in-our)
		- Prerequisite: [ARCHES: AI Research Considerations for Human Existential Safety](http://acritch.com/arches/)
	- Complex systems, economics, finance, business
- - -
Also [[Try Anki]]
Also read [[Research as a Stochastic Decision Process]]
Also read [[Research Taste Exercises]]
Also read [How To Get Into Independent Research On Alignment/Agency](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency)
- - -
## **Become comfortable working in a pre-paradigmatic field**
AI safety is often called a [pre-paradigmatic field](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency#Preparadigmicity), where there aren’t established frameworks or an established canon of facts.
> RL, NLP, and robustness are less pre-paradigmatic.
- find an experienced mentor
- stuck on a problem? divide and conquer
- increase credibility by coming up with grounded examples
- theories should be generalized, not ad-hoc
- try to apply knowledge from broad range of fields to solve problems
- while working on small problems occasionally zoom out to review high-level goals
## Formalizing Intuitions
- Translate informal models into math or code
- This helps with precisely defining and testing theories/intuitions
## Making deliverables
- In order to make independent research, readable there should deliverables such as
	- blog posts
	- github repos
	- research proposals
	- anything that can be put on a portfolio
- Quality over quantity of work
- - - 
- Also read [[the_sense_of_style.pdf|Pinker’s 'The Sense of Style']]
- Also watch [LEADERSHIP LAB: The Craft of Writing Effectively ](https://www.youtube.com/@UChicagoSSD)
- - -
## Specific paths taken by people
### John Wentworth
- Nature of [Agency](https://plato.stanford.edu/entries/agency/)
	- [[Agency and Agents]]